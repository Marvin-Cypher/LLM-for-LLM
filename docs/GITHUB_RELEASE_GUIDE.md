# GitHub Release Upload Guide

## Step-by-Step Instructions to Upload Large Result Files

GitHub's secret scanning is blocking our `git push` because the large result JSON files contain text patterns that look like API keys. The solution: **Upload files directly via GitHub Releases** (bypasses git push scanning).

---

## üìã Step 1: Prepare Files to Upload

The following files need to be uploaded via GitHub Releases:

### Required Files (Total ~40 MB):
```
results/phase1_final.json                          (~23 MB)
results/phase2_final.json                          (~10 MB)
results/falsereject_benchmark_final.json           (~4 MB)
batch_evaluation_jobs/results/appropriateness_results.jsonl  (~2 MB)
batch_evaluation_jobs/results/actionability_results.jsonl    (~500 KB)
reports/paper/paper_draft.md                       (~50 KB)
reports/paper/overleaf/main.tex                    (~30 KB)
```

### Optional (Nice to Have):
```
reports/paper/overleaf_package.zip                 (Complete LaTeX package)
reports/benchmark_report_20251106_130418.md        (Latest analysis report)
```

---

## üì¶ Step 2: Create a Release on GitHub

1. **Go to your repository**: https://github.com/Marvin-Cypher/LLM-for-LLM

2. **Click on "Releases"** (right sidebar, below "About")
   - Or go directly to: https://github.com/Marvin-Cypher/LLM-for-LLM/releases

3. **Click "Create a new release"** button

4. **Fill in the release details**:

   **Tag version**: `v1.0.0`

   **Release title**: `Legal LLM Benchmark - Complete Results (v1.0)`

   **Description**: Copy this text:
   ```markdown
   ## üéØ LLMs for LLMs: Legal Large Language Models Benchmark - Complete Results

   This release contains the full benchmark results for evaluating 10 LLMs across 163 legal tasks.

   ### üìä Key Findings

   - **üèÜ Top Performer**: GPT-5 (9.17/10, 0% over-refusal)
   - **üíé Best Value**: Qwen 2.5 72B (8.89/10, 0% over-refusal)
   - **‚ö†Ô∏è Critical Safety Issue**: GPT-OSS-120B (95.8%) and O3-Mini (87.5%) show catastrophic over-refusal

   ### üìÅ Files Included

   **Benchmark Results:**
   - `phase1_final.json` - Phase 1: 100 Q&A questions √ó 10 models
   - `phase2_final.json` - Phase 2: 39 contract tasks √ó 10 models
   - `falsereject_benchmark_final.json` - FalseReject: 24 over-refusal tests √ó 10 models

   **Evaluation Data:**
   - `appropriateness_results.jsonl` - LLM-as-judge appropriateness scores
   - `actionability_results.jsonl` - Actionability evaluation scores

   **Academic Paper:**
   - `paper_draft.md` - Full paper draft (Markdown)
   - `main.tex` - LaTeX source for submission
   - `overleaf_package.zip` - Complete Overleaf package with figures

   ### üîÑ Reproducibility

   All results can be reproduced using the scripts in this repository:
   ```bash
   python3 scripts/run_falsereject_benchmark.py
   python3 scripts/regenerate_124qa_figures.py
   ```

   ### üìñ Citation

   If you use this benchmark in your research, please cite:
   ```bibtex
   @misc{legal-llm-benchmark-2025,
     title={LLMs for LLMs: Evaluating Large Language Models for Legal Practice},
     author={Your Name},
     year={2025},
     url={https://github.com/Marvin-Cypher/LLM-for-LLM}
   }
   ```

   ### üìß Contact

   For questions or collaboration: [Your Email]

   ---

   **Note**: These files are hosted via GitHub Releases because they are too large for git push and contain text patterns that trigger GitHub's secret scanning (false positives from model-generated legal text).
   ```

5. **Upload files**:
   - Scroll down to "Attach binaries by dropping them here or selecting them"
   - Drag and drop all 7 files listed above
   - Or click to browse and select them

6. **Click "Publish release"**

---

## üìù Step 3: Update README with Download Links

After publishing the release, GitHub will give you permanent download URLs. Update your README.md:

```markdown
## üì• Download Benchmark Results

Due to file size, the complete benchmark results are available via GitHub Releases:

**[Download v1.0.0 Release ‚Üí](https://github.com/Marvin-Cypher/LLM-for-LLM/releases/tag/v1.0.0)**

### Quick Download Links:

- [phase1_final.json](https://github.com/Marvin-Cypher/LLM-for-LLM/releases/download/v1.0.0/phase1_final.json) - Phase 1 Results (23 MB)
- [phase2_final.json](https://github.com/Marvin-Cypher/LLM-for-LLM/releases/download/v1.0.0/phase2_final.json) - Phase 2 Results (10 MB)
- [falsereject_benchmark_final.json](https://github.com/Marvin-Cypher/LLM-for-LLM/releases/download/v1.0.0/falsereject_benchmark_final.json) - FalseReject Results (4 MB)
- [Full Paper (PDF)](https://github.com/Marvin-Cypher/LLM-for-LLM/releases/download/v1.0.0/paper_draft.pdf) - Academic Paper

After downloading, place files in the corresponding directories:
```bash
# Download and place in correct locations
wget https://github.com/Marvin-Cypher/LLM-for-LLM/releases/download/v1.0.0/phase1_final.json -P results/
wget https://github.com/Marvin-Cypher/LLM-for-LLM/releases/download/v1.0.0/phase2_final.json -P results/
wget https://github.com/Marvin-Cypher/LLM-for-LLM/releases/download/v1.0.0/falsereject_benchmark_final.json -P results/
```
```

---

## ‚úÖ Step 4: Verify Everything Works

After publishing:

1. Visit your release page: https://github.com/Marvin-Cypher/LLM-for-LLM/releases/tag/v1.0.0
2. Click each download link to verify files are accessible
3. Test the wget commands in your README
4. Share the repository link with collaborators!

---

## üéâ Done!

Your repository is now **publication-ready** with:
- ‚úÖ Clean, professional GitHub page
- ‚úÖ Complete dataset and code
- ‚úÖ Downloadable results via Releases
- ‚úÖ Academic paper and figures
- ‚úÖ Reproduction instructions

**Repository URL**: https://github.com/Marvin-Cypher/LLM-for-LLM

---

## üîß Alternative: HuggingFace (Optional)

If you want better discoverability in the ML community, you can ALSO upload to HuggingFace:

1. Go to https://huggingface.co/new-dataset
2. Create dataset: `your-username/legal-llm-benchmark`
3. Upload the same files
4. Add link in your GitHub README: "Also available on ü§ó HuggingFace"

This gives you:
- DOI for citation
- Better search visibility
- ML community engagement
- Automatic dataset viewer
